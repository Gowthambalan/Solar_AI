{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ba31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chardet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6836bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import gzip\n",
    "# import chardet\n",
    "\n",
    "class FileValidator:\n",
    "    def __init__(self, max_size_mb=5):\n",
    "        \"\"\"Initialize validator with size limit (default 5MB).\"\"\"\n",
    "        self.max_size = max_size_mb * 1024 * 1024\n",
    "\n",
    "    def validate(self, file_path):\n",
    "        \"\"\"Validate file step by step.\"\"\"\n",
    "        results = {\n",
    "            \"status\": \"PASS\",\n",
    "            \"reason\": None,\n",
    "            \"error_path\": None\n",
    "        }\n",
    "\n",
    "        # Step 1: Extension check\n",
    "        if not self._is_valid_extension(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"NOT_JSONL\"\n",
    "            results[\"error_path\"] = \"/error/not_jsonl/\"\n",
    "            return results\n",
    "\n",
    "        # # Step 2: Encoding check (UTF-8)\n",
    "        # if not self._is_valid_utf8(file_path):\n",
    "        #     results[\"status\"] = \"FAIL\"\n",
    "        #     results[\"reason\"] = \"BAD_ENCODING\"\n",
    "        #     results[\"error_path\"] = \"/error/bad_encoding/\"\n",
    "        #     return results\n",
    "\n",
    "        # Step 3: Size and empty check\n",
    "        if not self._is_valid_size(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"EMPTY_FILE or TOO_LARGE\"\n",
    "            results[\"error_path\"] = \"/error/empty_file/\"  # or /error/too_large/\n",
    "            return results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _is_valid_extension(self, file_path):\n",
    "        allowed_ext = [\".jsonl\", \".ndjson\", \".jsonl.gz\", \".ndjson.gz\",\".json\"]\n",
    "        return any(file_path.endswith(ext) for ext in allowed_ext)\n",
    "\n",
    "    # def _is_valid_utf8(self, file_path):\n",
    "    #     try:\n",
    "    #         # Handle gzipped files\n",
    "    #         open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "    #         with open_func(file_path, 'rb') as f:\n",
    "    #             raw_data = f.read(4096)\n",
    "    #             result = chardet.detect(raw_data)\n",
    "    #             return result['encoding'] == 'utf-8'\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Encoding check error: {e}\")\n",
    "    #         return False\n",
    "\n",
    "    def _is_valid_size(self, file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        if size == 0:\n",
    "            print(\"File is empty\")\n",
    "            return False\n",
    "        if size > self.max_size:\n",
    "            print(f\"File too large: {size} bytes\")\n",
    "            return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60028ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'PASS', 'reason': None, 'error_path': None}\n"
     ]
    }
   ],
   "source": [
    "validator = FileValidator(max_size_mb=10)\n",
    "\n",
    "file_path = r\"D:\\AI Projects\\powerthon\\powerthon_AI\\external_data\\new_output_jsons\\new_output_jsons\\api_raw_json_analytics_20251017\\861850060226525_20251017161523.json\"  # replace with your test file\n",
    "result = validator.validate(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05749470",
   "metadata": {},
   "source": [
    "convert ascii into utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c35229",
   "metadata": {},
   "source": [
    "hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# ðŸ”¹ Step 1: Give your JSON file path here\n",
    "file_path = r\"D:\\AI Projects\\powerthon\\powerthon_AI\\external_data\\new_output_jsons\\new_output_jsons\\api_raw_json_analytics_20251017\\861850060226657_20251017180005_utf8.json\"   # â† replace with your file path\n",
    "\n",
    "# ðŸ”¹ Step 2: Read file in binary mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# ðŸ”¹ Step 3: Detect encoding\n",
    "result = chardet.detect(raw_data)\n",
    "detected_encoding = result['encoding']\n",
    "confidence = result['confidence']\n",
    "\n",
    "print(f\"ðŸ“˜ Detected Encoding: {detected_encoding}\")\n",
    "print(f\"ðŸ” Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "# ðŸ”¹ Step 4: Check UTF-8 status\n",
    "if detected_encoding and detected_encoding.lower() == 'utf-8':\n",
    "    print(\"âœ… The file is UTF-8 encoded.\")\n",
    "else:\n",
    "    print(\"âš ï¸ The file is NOT UTF-8 encoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08726e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ðŸ”¹ Give the same file path\n",
    "file_path = r\"D:\\AI Projects\\powerthon\\powerthon_AI\\external_data\\new_output_jsons\\new_output_jsons\\api_raw_json_analytics_20251017\\861850060226657_20251017180005.json\"   # â† same file as above\n",
    "\n",
    "# ðŸ”¹ Read and detect encoding again\n",
    "with open(file_path, 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "import chardet\n",
    "detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "# ðŸ”¹ Convert and save as UTF-8\n",
    "try:\n",
    "    text = raw_data.decode(detected_encoding)\n",
    "    data = json.loads(text)  # validate JSON format\n",
    "\n",
    "    new_path = file_path.replace(\".json\", \"_utf8.json\")\n",
    "    with open(new_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… File successfully converted and saved as: {new_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Conversion failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ff4b0",
   "metadata": {},
   "source": [
    "one by one adding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "   \n",
    "\n",
    "class FileValidator:\n",
    "    def __init__(self, max_size_mb=5):\n",
    "        \"\"\"Initialize validator with size limit (default 5MB).\"\"\"\n",
    "        self.max_size = max_size_mb * 1024 * 1024\n",
    "\n",
    "    def validate(self, file_path):\n",
    "        \"\"\"Validate file step by step.\"\"\"\n",
    "        results = {\n",
    "            \"status\": \"PASS\",\n",
    "            \"reason\": None,\n",
    "            \"error_path\": None,\n",
    "            \"skipped_lines\": 0\n",
    "        }\n",
    "\n",
    "        # Step 1: Extension check\n",
    "        if not self._is_valid_extension(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"NOT_JSONL\"\n",
    "            results[\"error_path\"] = \"/error/not_jsonl/\"\n",
    "            return results\n",
    "\n",
    "        # Step 2: (Optional) Encoding check\n",
    "        # if not self._is_valid_utf8(file_path):\n",
    "        #     results[\"status\"] = \"FAIL\"\n",
    "        #     results[\"reason\"] = \"BAD_ENCODING\"\n",
    "        #     results[\"error_path\"] = \"/error/bad_encoding/\"\n",
    "        #     return results\n",
    "\n",
    "        # Step 3: Size and empty check\n",
    "        if not self._is_valid_size(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"EMPTY_FILE or TOO_LARGE\"\n",
    "            results[\"error_path\"] = \"/error/empty_file/\"\n",
    "            return results\n",
    "\n",
    "        # Step 4: Record-level validation (skip blank lines, check JSON)\n",
    "        record_check = self._validate_records(file_path)\n",
    "        if record_check[\"status\"] == \"FAIL\":\n",
    "            return record_check  # return early if JSON parse fails\n",
    "\n",
    "        results[\"skipped_lines\"] = record_check[\"skipped_lines\"]\n",
    "        return results\n",
    "\n",
    "    # -----------------------\n",
    "    # Helper Functions\n",
    "    # -----------------------\n",
    "\n",
    "    def _is_valid_extension(self, file_path):\n",
    "        allowed_ext = [\".jsonl\", \".ndjson\", \".jsonl.gz\", \".ndjson.gz\", \".json\"]\n",
    "        return any(file_path.endswith(ext) for ext in allowed_ext)\n",
    "\n",
    "    # def _is_valid_utf8(self, file_path):\n",
    "    #     try:\n",
    "    #         open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "    #         with open_func(file_path, 'rb') as f:\n",
    "    #             raw_data = f.read(4096)\n",
    "    #             result = chardet.detect(raw_data)\n",
    "    #             return result['encoding'] == 'utf-8'\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Encoding check error: {e}\")\n",
    "    #         return False\n",
    "\n",
    "    def _is_valid_size(self, file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        if size == 0:\n",
    "            print(\"File is empty\")\n",
    "            return False\n",
    "        if size > self.max_size:\n",
    "            print(f\"File too large: {size} bytes\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _validate_records(self, file_path):\n",
    "        \"\"\"Skip blank lines and validate JSON structure line by line.\"\"\"\n",
    "        results = {\n",
    "            \"status\": \"PASS\",\n",
    "            \"reason\": None,\n",
    "            \"error_path\": None,\n",
    "            \"skipped_lines\": 0\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line_num, line in enumerate(f, start=1):\n",
    "                    stripped = line.strip()\n",
    "\n",
    "                    # Step 6: Skip blank or whitespace-only lines\n",
    "                    if not stripped:\n",
    "                        results[\"skipped_lines\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Step 7: Strict JSON parsing\n",
    "                    # try:\n",
    "                    #     json.loads(stripped)\n",
    "                    # except json.JSONDecodeError as e:\n",
    "                    #     print(f\"Line {line_num} JSON parse error: {e}\")\n",
    "                    #     results[\"status\"] = \"FAIL\"\n",
    "                    #     results[\"reason\"] = \"LINE_PARSE\"\n",
    "                    #     results[\"error_path\"] = \"/error/line_parse/\"\n",
    "                    #     return results  # stop checking further\n",
    "        except Exception as e:\n",
    "            print(f\"Record validation error: {e}\")\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"RECORD_VALIDATION_ERROR\"\n",
    "            results[\"error_path\"] = \"/error/record_validation/\"\n",
    "            return results\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97baac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'PASS', 'reason': None, 'error_path': None, 'skipped_lines': 0}\n"
     ]
    }
   ],
   "source": [
    "validator = FileValidator(max_size_mb=10)\n",
    "\n",
    "file_path = r\"D:\\AI Projects\\powerthon\\powerthon_AI\\external_data\\new_output_jsons\\new_output_jsons\\api_raw_json_analytics_20251017\\861850060226525_20251017163031.json\"  # replace with your test file\n",
    "result = validator.validate(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf675f",
   "metadata": {},
   "source": [
    "#new one ny one adding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil import parser as date_parser\n",
    "# import gzip\n",
    "# import chardet\n",
    "\n",
    "class FileValidator:\n",
    "    def __init__(self, max_size_mb=5):\n",
    "        \"\"\"Initialize validator with size limit (default 5MB).\"\"\"\n",
    "        self.max_size = max_size_mb * 1024 * 1024\n",
    "\n",
    "    def validate(self, file_path):\n",
    "        \"\"\"Validate file step by step.\"\"\"\n",
    "        results = {\n",
    "            \"status\": \"PASS\",\n",
    "            \"reason\": None,\n",
    "            \"error_path\": None,\n",
    "            \"skipped_lines\": 0\n",
    "        }\n",
    "\n",
    "        # Step 1: Extension check\n",
    "        if not self._is_valid_extension(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"NOT_JSONL\"\n",
    "            results[\"error_path\"] = \"/error/not_jsonl/\"\n",
    "            return results\n",
    "\n",
    "        # Step 2: (Optional) Encoding check (disabled)\n",
    "        # if not self._is_valid_utf8(file_path):\n",
    "        #     results[\"status\"] = \"FAIL\"\n",
    "        #     results[\"reason\"] = \"BAD_ENCODING\"\n",
    "        #     results[\"error_path\"] = \"/error/bad_encoding/\"\n",
    "        #     return results\n",
    "\n",
    "        # Step 3: Size and empty check\n",
    "        if not self._is_valid_size(file_path):\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"EMPTY_FILE or TOO_LARGE\"\n",
    "            results[\"error_path\"] = \"/error/empty_file/\"\n",
    "            return results\n",
    "\n",
    "        # Step 4â€“10: Record-level validation\n",
    "        record_check = self._validate_records(file_path)\n",
    "        if record_check[\"status\"] == \"FAIL\":\n",
    "            return record_check\n",
    "\n",
    "        results[\"skipped_lines\"] = record_check[\"skipped_lines\"]\n",
    "        return results\n",
    "\n",
    "    # -----------------------\n",
    "    # Helper Functions\n",
    "    # -----------------------\n",
    "\n",
    "    def _is_valid_extension(self, file_path):\n",
    "        allowed_ext = [\".jsonl\", \".ndjson\", \".jsonl.gz\", \".ndjson.gz\", \".json\"]\n",
    "        return any(file_path.endswith(ext) for ext in allowed_ext)\n",
    "    \n",
    "    # def _is_valid_utf8(self, file_path):\n",
    "    #     try:\n",
    "    #         open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "    #         with open_func(file_path, 'rb') as f:\n",
    "    #             raw_data = f.read(4096)\n",
    "    #             result = chardet.detect(raw_data)\n",
    "    #             return result['encoding'] == 'utf-8'\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Encoding check error: {e}\")\n",
    "    #         return False\n",
    "\n",
    "    def _is_valid_size(self, file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        if size == 0:\n",
    "            print(\"File is empty\")\n",
    "            return False\n",
    "        if size > self.max_size:\n",
    "            print(f\"File too large: {size} bytes\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # -----------------------\n",
    "    # Record-Level Validation\n",
    "    # -----------------------\n",
    "\n",
    "    def _validate_records(self, file_path):\n",
    "        \"\"\"Skip blank lines, check JSON validity, type coercion, and timestamp normalization.\"\"\"\n",
    "        results = {\n",
    "            \"status\": \"PASS\",\n",
    "            \"reason\": None,\n",
    "            \"error_path\": None,\n",
    "            \"skipped_lines\": 0\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line_num, line in enumerate(f, start=1):\n",
    "                    stripped = line.strip()\n",
    "\n",
    "                    # Step 6: Skip blank lines\n",
    "                    if not stripped:\n",
    "                        results[\"skipped_lines\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Step 7: JSON structure validation\n",
    "                    try:\n",
    "                        record = json.loads(stripped)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Line {line_num} JSON parse error: {e}\")\n",
    "                        results[\"status\"] = \"FAIL\"\n",
    "                        results[\"reason\"] = \"LINE_PARSE\"\n",
    "                        results[\"error_path\"] = \"/error/line_parse/\"\n",
    "                        return results\n",
    "\n",
    "                    # Step 9: Type coercion check (numeric fields)\n",
    "                    if not self._check_type_coercion(record, line_num, results):\n",
    "                        return results\n",
    "\n",
    "                    # Step 10: Timestamp normalization & validation\n",
    "                    if not self._check_timestamps(record, line_num, results):\n",
    "                        return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Record validation error: {e}\")\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"RECORD_VALIDATION_ERROR\"\n",
    "            results[\"error_path\"] = \"/error/record_validation/\"\n",
    "            return results\n",
    "\n",
    "        return results\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 9 â€“ Type Coercion\n",
    "    # -----------------------\n",
    "\n",
    "    def _check_type_coercion(self, record, line_num, results):\n",
    "        \"\"\"Try coercing numeric-like values to int/float. If fails â†’ DQ failed.\"\"\"\n",
    "        try:\n",
    "            for key, value in record.items():\n",
    "                # Skip non-string types\n",
    "                if not isinstance(value, str):\n",
    "                    continue\n",
    "\n",
    "                # Check if string looks numeric\n",
    "                if value.replace(\".\", \"\", 1).isdigit():\n",
    "                    try:\n",
    "                        # Try float or int conversion\n",
    "                        _ = float(value) if \".\" in value else int(value)\n",
    "                    except Exception:\n",
    "                        print(f\"Line {line_num} DQ failed - cannot coerce {key}='{value}'\")\n",
    "                        results[\"status\"] = \"FAIL\"\n",
    "                        results[\"reason\"] = \"DQ_FAILED\"\n",
    "                        results[\"error_path\"] = \"/error/dq_failed/\"\n",
    "                        return False\n",
    "        except Exception as e:\n",
    "            print(f\"Type coercion error on line {line_num}: {e}\")\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"DQ_FAILED\"\n",
    "            results[\"error_path\"] = \"/error/dq_failed/\"\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 10 â€“ Timestamp Check\n",
    "    # -----------------------\n",
    "\n",
    "    def _check_timestamps(self, record, line_num, results):\n",
    "        \"\"\"Check if timestamp fields can be parsed & normalized to UTC ISO-8601.\"\"\"\n",
    "        try:\n",
    "            for key, value in record.items():\n",
    "                if not isinstance(value, str):\n",
    "                    continue\n",
    "\n",
    "                # Check keys that look like timestamps or ISO strings\n",
    "                if any(term in key.lower() for term in [\"time\", \"date\", \"timestamp\",\"ReadingTimeStamp\",\"stage_timestamp\",\"alidation_timestamp\",\"processed_at\"]):\n",
    "                    try:\n",
    "                        parsed_time = date_parser.parse(value)\n",
    "                        _ = parsed_time.astimezone().isoformat()  # convert to UTC ISO-8601\n",
    "                    except Exception:\n",
    "                        print(f\"Line {line_num} invalid timestamp for {key}: {value}\")\n",
    "                        results[\"status\"] = \"FAIL\"\n",
    "                        results[\"reason\"] = \"INVALID_TIMESTAMP\"\n",
    "                        results[\"error_path\"] = \"/error/timestamp_invalid/\"\n",
    "                        return False\n",
    "        except Exception as e:\n",
    "            print(f\"Timestamp normalization error on line {line_num}: {e}\")\n",
    "            results[\"status\"] = \"FAIL\"\n",
    "            results[\"reason\"] = \"INVALID_TIMESTAMP\"\n",
    "            results[\"error_path\"] = \"/error/timestamp_invalid/\"\n",
    "            return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b83ee4",
   "metadata": {},
   "source": [
    "NEW VALIDATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
